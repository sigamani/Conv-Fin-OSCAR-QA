model_name_or_path: "meta-llama/Llama-2-7b-hf"
output_dir: "checkpoints/oscar-lora"
dataset_path: "data/curriculum_generated.jsonl"

# Training hyperparameters
# num_train_epochs: 3
# per_device_train_batch_size: 8
# gradient_accumulation_steps: 2
# learning_rate: 2e-4
# lr_scheduler_type: "cosine"
# warmup_ratio: 0.1
#
# # LoRA config
# use_lora: true
# lora_rank: 8
# lora_alpha: 16
# lora_dropout: 0.05
